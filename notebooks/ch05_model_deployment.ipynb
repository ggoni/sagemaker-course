{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performing Inference with a Trained Model\n",
    "\n",
    "* Goals\n",
    "    * Using a model trained on Day 1:\n",
    "        * Host the model as an **endpoint** for online inference.\n",
    "        * Use **Batch Transform** to perform batch inference using the model.\n",
    "        * Demonstrate how to enable autoscaling.\n",
    "* Code adapted from the [scikit_learn_iris](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/scikit_learn_iris) and [batch_transform_pca_dbscan_movie_clusters](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_batch_transform/introduction_to_batch_transform/batch_transform_pca_dbscan_movie_clusters.ipynb) sample notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup\n",
    "\n",
    "Change into the notebooks directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/sagemaker-workshop-420/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /root/sagemaker-workshop-420/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets create our Sagemaker session and role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::209970524256:role/service-role/AmazonSageMaker-ExecutionRole-20200414T065516\n"
     ]
    }
   ],
   "source": [
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Host a pretrained Sklearn Model for Online Inference\n",
    "\n",
    "We will deploy use the sklearn model we trained on the Iris dataset as a hosted endpoint for online inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artifacts will be written to/read from s3://sagemaker-workshop-420/iris\n"
     ]
    }
   ],
   "source": [
    "BUCKET = 'sagemaker-workshop-420'\n",
    "PREFIX = 'iris'\n",
    "\n",
    "LOCAL_DATA_DIRECTORY = f'../data/{PREFIX}'\n",
    "\n",
    "print(f\"\\nArtifacts will be written to/read from s3://{BUCKET}/{PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Locate the S3 path of the serialized model\n",
    "\n",
    "To utilize a trained model, we need to pass in the S3 URI of the serialized model artifact. We can find this by looking through the metadata of the Training Job. This can be done in the SageMaker Studio UI or in the AWS SageMaker console under Training Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_path = 's3://sagemaker-workshop-420/iris/sagemaker-scikit-learn-2020-04-15-22-35-55-023/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Initialize a `sklearn.model.Model` that can be deployed as an `Endpoint`\n",
    "\n",
    "Next we create a `sagemaker.sklearn.SKLearnModel` object which allows us to deploy our pretrained model as an `Endpoint`. See the `sagemaker.sklearn.SKLearnModel` [API reference](https://sagemaker.readthedocs.io/en/stable/sagemaker.sklearn.html#sagemaker.sklearn.model.SKLearnModel) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = SKLearnModel(model_data=model_s3_path,\n",
    "                             role=role, \n",
    "                             entry_point='../scripts/sklearn_iris.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Deploy the `Model` as an `Endpoint` and return a `RealtimePredictor` object.\n",
    "\n",
    "See the `sagemaker.predictor.RealTimePredictor` [API reference](https://sagemaker.readthedocs.io/en/stable/predictors.html) for more details.\n",
    "\n",
    "**NOTE: This takes about 6-8 minutes to return.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "sklearn_predictor = sklearn_model.deploy(initial_instance_count=1,\n",
    "                                         instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Perform Inference\n",
    "\n",
    "Next we can load in data and use our hosted endpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_preds = sklearn_predictor.predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(iris_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Autoscaling\n",
    "\n",
    "**To configure autoscaling for a model using the console**\n",
    "\n",
    "1. Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/.\n",
    "2. In the navigation pane, choose **Endpoints**.\n",
    "3. Choose the endpoint that you want to configure.\n",
    "4. For **Endpoint runtime settings**, choose the model variant that you want to configure.\n",
    "5. For **Endpoint runtime settings**, choose **Configure autoscaling**. The **Configure variant automatic scaling** page appears.\n",
    "6. For **Minimum capacity**, type the minimum number of instances that you want the scaling policy to maintain. At least 1 instance is required.\n",
    "7. For **Maximum capacity**, type the maximum number of instances that you want the scaling policy to maintain.\n",
    "8. For the **target value**, type the average number of invocations per instance per minute for the model. To determine this value, follow the guidelines in Load testing. Application Auto Scaling adds or removes instances to keep the metric close to the value that you specify.\n",
    "9. For **Scale-in cool down (seconds)** and **Scale-out cool down (seconds)**, type the number seconds for each cool down period. Assuming that the order in the list is based on either most important to less important of first applied to last applied.\n",
    "10. Select **Disable scale in** to prevent the scaling policy from deleting variant instances if you want to ensure that your variant scales out to address increased traffic, but are not concerned with removing instances to reduce costs when traffic decreases, disable scale-in activities. Scale-out activities are always enabled so that the scaling policy can create endpoint instances as needed.\n",
    "11. Choose **Save**.\n",
    "\n",
    "More details, including how to define a custom scaling policy, can be found in the [Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-add-console.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint cleanup <a class=\"anchor\" id=\"endpoint_cleanup\"></a>\n",
    "\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform batch inference using SageMaker Batch Transform \n",
    "\n",
    "We can also use the trained model for asynchronous batch inference on S3 data using SageMaker Batch Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Prepare Input Data\n",
    "\n",
    "We will use the training data file we uploaded yesterday for inference.\n",
    "\n",
    "```\n",
    "An example of input file content:\n",
    "                Record1-Attribute1, Record1-Attribute2, Record1-Attribute3, ..., Record1-AttributeM\n",
    "                Record2-Attribute1, Record2-Attribute2, Record2-Attribute3, ..., Record2-AttributeM\n",
    "                Record3-Attribute1, Record3-Attribute2, Record3-Attribute3, ..., Record3-AttributeM\n",
    "                ...\n",
    "                RecordN-Attribute1, RecordN-Attribute2, RecordN-Attribute3, ..., RecordN-AttributeM\n",
    "```         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/iris/iris_batch_inference.csv\n"
     ]
    }
   ],
   "source": [
    "iris_X = pd.DataFrame(iris.data)\n",
    "\n",
    "local_file_path = f'{LOCAL_DATA_DIRECTORY}/iris_batch_inference.csv'\n",
    "iris_X.to_csv(local_file_path, header=False, index=False)\n",
    "\n",
    "print(local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2\n",
      "4.9,3.0,1.4,0.2\n",
      "4.7,3.2,1.3,0.2\n",
      "4.6,3.1,1.5,0.2\n",
      "5.0,3.6,1.4,0.2\n",
      "5.4,3.9,1.7,0.4\n",
      "4.6,3.4,1.4,0.3\n",
      "5.0,3.4,1.5,0.2\n",
      "4.4,2.9,1.4,0.2\n",
      "4.9,3.1,1.5,0.1\n"
     ]
    }
   ],
   "source": [
    "!head ../data/iris/iris_batch_inference.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-workshop-420/iris/iris_batch_inference.csv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_data = sagemaker_session.upload_data(\n",
    "    local_file_path,\n",
    "    bucket=BUCKET,\n",
    "    key_prefix=PREFIX)\n",
    "\n",
    "inference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Initialize a `Tranformer` object\n",
    "\n",
    "See the `sagemaker.transformer.Transformer` [API reference](https://sagemaker.readthedocs.io/en/stable/transformer.html) for more details.\n",
    "\n",
    "* Hardware specification (instance count and type). Prediction is embarassingly parallel, so feel free to test this with multiple instances, but since our dataset is not enormous, we'll stick to one.\n",
    "* `strategy`: Which determines how records should be batched into each prediction request within the batch transform job. 'MultiRecord' may be faster, but some use cases may require 'SingleRecord'.\n",
    "* `assemble_with`: Which controls how predictions are output. 'None' does not perform any special processing, 'Line' places each prediction on it's own line.\n",
    "* `output_path`: The S3 location for batch transform to be output. Note, file(s) will be named with '.out' suffixed to the input file(s) names. In our case this will be 'train.csv.out'. Note that in this case, multiple batch transform runs will overwrite existing values unless this is updated appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_transformer = sklearn_model.transformer(instance_count=1,\n",
    "                                                instance_type='ml.m4.xlarge',\n",
    "                                                strategy='MultiRecord',\n",
    "                                                assemble_with='Line',\n",
    "                                                output_path=f\"s3://{BUCKET}/{PREFIX}/transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Run Transform Job \n",
    "\n",
    "Using the Transformer, run a transform job on the S3 input data.\n",
    "\n",
    "A critical parameter to set properly here is `split_type`. Since we are using CSV, we'll specify 'Line', which ensures we only pass one line at a time to our algorithm for prediction. Had we not specified this, we'd attempt to pass all lines in our file, which would exhaust our transformer instance's memory.\n",
    "\n",
    "Note: Here we pass the S3 path as input rather than input we use in .fit().\n",
    "\n",
    "**NOTE: This takes about 3-5 minutes to return.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for transform job: sagemaker-scikit-learn-2020-04-16-11-29-2020-04-16-11-29-58-815\n",
      "...................\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-iris\n",
      "  Building wheel for sklearn-iris (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sklearn-iris (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-iris: filename=sklearn_iris-1.0.0-py2.py3-none-any.whl size=7005 sha256=779d88c57bd20a2ee7153bf24a0149d458194f70ee36bee893e4875ddd38caf9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jby6p02t/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-iris\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-iris\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-iris-1.0.0\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m[2020-04-16 11:32:57 +0000] [38] [INFO] Starting gunicorn 19.9.0\u001b[0m\n",
      "\u001b[34m[2020-04-16 11:32:57 +0000] [38] [INFO] Listening at: unix:/tmp/gunicorn.sock (38)\u001b[0m\n",
      "\u001b[34m[2020-04-16 11:32:57 +0000] [38] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-04-16 11:32:57 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2020-04-16 11:32:57 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2020-04-16 11:32:57 +0000] [46] [INFO] Booting worker with pid: 46\u001b[0m\n",
      "\u001b[34m[2020-04-16 11:32:57 +0000] [47] [INFO] Booting worker with pid: 47\u001b[0m\n",
      "\n",
      "\u001b[34m2020-04-16 11:33:26,720 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [16/Apr/2020:11:33:27 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [16/Apr/2020:11:33:27 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [16/Apr/2020:11:33:27 +0000] \"POST /invocations HTTP/1.1\" 200 750 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2020-04-16 11:33:26,720 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [16/Apr/2020:11:33:27 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [16/Apr/2020:11:33:27 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [16/Apr/2020:11:33:27 +0000] \"POST /invocations HTTP/1.1\" 200 750 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2020-04-16T11:33:27.230:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start a transform job and wait for it to finish\n",
    "sklearn_transformer.transform(data=inference_data,\n",
    "                              content_type='text/csv',\n",
    "                              split_type='Line')\n",
    "\n",
    "print('Waiting for transform job: ' + sklearn_transformer.latest_transform_job.job_name)\n",
    "sklearn_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Check Output Data\n",
    "\n",
    "After the transform job has completed, download the output data from S3. For each file **FILENAME** in the input data, we have a corresponding file **FILENAME.out** containing the predicted labels from each input row. We can compare the predicted labels to the true labels saved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-workshop-420/iris/transform'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output = sklearn_transformer.output_path\n",
    "batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local filesystem\n",
    "boto_session.client('s3').download_file(\n",
    "    Bucket=BUCKET,\n",
    "    Key=f\"{PREFIX}/transform/iris_batch_inference.csv.out\",\n",
    "    Filename=f'{LOCAL_DATA_DIRECTORY}/iris_batch_inference.csv.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/iris/iris_batch_inference.csv.out'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{LOCAL_DATA_DIRECTORY}/iris_batch_inference.csv.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "!cat ../data/iris/iris_batch_inference.csv.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
